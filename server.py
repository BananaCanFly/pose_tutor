import torch
from fastapi import FastAPI, File, UploadFile, Form
import cv2
import numpy as np
import mediapipe as mp
import time
import os

from realtime_extractor import PoseExtractor
from CompositionAnalyzer import compute_bbox, analyze_crop_and_zoom, compute_bbox_standard, suggest_orientation_multi, \
    compute_bbox_by_mode

# ç¡®ä¿æœ‰ä¸ªæ–‡ä»¶å¤¹å­˜è¿™äº›â€œå¤±è´¥â€çš„å›¾ç‰‡
if not os.path.exists("debug_frames"):
    os.makedirs("debug_frames")

app = FastAPI()

# åˆå§‹åŒ– MediaPipe
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)
from ultralytics import YOLO

# åŠ è½½æ¨¡å‹ (yolov8n æ˜¯æœ€è½»é‡çš„)
yolo_model = YOLO('yolov8n.pt')

# yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
yolo_model.classes = [0]
yolo_model.conf = 0.4
yolo_model.iou = 0.5
yolo_model.max_det = 1

# @app.post("/analyze")
# async def analyze_pose(
#         file: UploadFile = File(...),
#         selected_pose: str = Form(...)
# ):
#     start_time = time.time()
#
#     # 1. è¯»å–å¹¶è§£ç å›¾ç‰‡
#     contents = await file.read()
#     nparr = np.frombuffer(contents, np.uint8)
#     image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
#
#     if image is None:
#         print("âŒ [é”™è¯¯] æ— æ³•è§£ç å›¾ç‰‡")
#         return {"success": False, "msg": "å›¾åƒè§£ç å¤±è´¥"}
#
#     h, w, _ = image.shape
#     # print(f"\nğŸ”” [æ–°è¯·æ±‚] å§¿æ€ç±»å‹: {selected_pose} | åˆ†è¾¨ç‡: {w}x{h}")
#
#     # 2. å§¿æ€æ£€æµ‹
#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
#
#     # cv2.imshow('Check image', image)
#     # cv2.waitKey(0)
#     # cv2.destroyAllWindows()
#
#     results = pose.process(image_rgb)
#
#     response_data = {
#         "success": True,
#         "scale": 1.0,
#         "bbox": [0, 0, 0, 0],
#         "center": [0.5, 0.5],
#         "suggestions": ["æœªæ£€æµ‹åˆ°äººç‰©"],
#         "user_points": [],
#     }
#
#     # 3. å¦‚æœæ£€æµ‹åˆ°éª¨æ¶
#     if results.pose_landmarks:
#         landmarks = results.pose_landmarks.landmark
#
#         bbox_result = compute_bbox_standard(image, PoseExtractor().extract_from_frame(image), yolo_model)
#         camera_suggestion = analyze_crop_and_zoom(image, PoseExtractor().extract_from_frame(image),
#                                                   yolo_model)
#
#         # cached_bbox = {
#         #     "center": bbox_result["target_center"],
#         #     "bbox": bbox_result["bbox"],
#         #     "scale": bbox_result["scale"],
#         # }
#
#         # æ•´ç†è¿”å›å†…å®¹
#         response_data["user_points"] = [[lm.x, lm.y] for lm in landmarks]
#         response_data["bbox"] = bbox_result["bbox"]
#         response_data["center"] = bbox_result["target_center"]
#         response_data["scale"] = bbox_result["scale"]
#         response_data["suggestions"] = camera_suggestion
#
#         # print(response_data)
#
#     else:
#         print("âš ï¸ [æ£€æµ‹å¤±è´¥] ç”»é¢ä¸­æ²¡æœ‰å‘ç°äºº")
#
#     end_time = time.time()
#     # print(f"â±ï¸ [è€—æ—¶] {(end_time - start_time) * 1000:.2f} ms")
#
#     return response_data
#
# @app.post("/analyze")
# async def get_photography_advice(
#         file: UploadFile = File(...)
# ):
#     # 1. è¯»å–å¹¶è§£ç å›¾ç‰‡
#     contents = await file.read()
#     nparr = np.frombuffer(contents, np.uint8)
#     frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
#
#     if frame is None:
#         print("âŒ [é”™è¯¯] æ— æ³•è§£ç å›¾ç‰‡")
#         return {"success": False, "msg": "å›¾åƒè§£ç å¤±è´¥"}
#
#     h_img, w_img = frame.shape[:2]
#
#     # --- æ­¥éª¤ 1: AI è¿è¡Œ ---
#     # YOLO æ£€æµ‹ (å¤šäºº/ç‰©ä½“)
#     results = yolo_model(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
#     people_df = results.pandas().xyxy[0]
#     people_df = people_df[people_df['class'] == 0]  # åªçœ‹äºº
#
#     # MediaPipe æ£€æµ‹ (å•äººéª¨éª¼)
#     # æ³¨æ„ï¼šå¦‚æœæ˜¯å¤šäººï¼Œè¿™é‡Œé€šå¸¸å–ç½®ä¿¡åº¦æœ€é«˜çš„æˆ–è€…é¢ç§¯æœ€å¤§çš„
#     mp_results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
#
#     # --- æ­¥éª¤ 2: é€»è¾‘è®¡ç®— ---
#     advice_list = []
#
#     if not people_df.empty:
#         # A. è®¡ç®—æ¨ªç«–å±å»ºè®®
#         orientation, o_reason = suggest_orientation_multi(people_df)
#         advice_list.append(f"å»ºè®®: {orientation} ({o_reason})")
#         return  {
#             "advice": advice_list,
#         }
#
#     return None


@app.post("/analyze")
async def analyze_pose(
        file: UploadFile = File(...),
        pose_type: str = Form("ç«™ç«‹"),
        view_mode: str = Form("å…¨èº«åƒ")
):
    start_time = time.time()

    # 1. è¯»å–å¹¶è§£ç å›¾ç‰‡
    contents = await file.read()
    nparr = np.frombuffer(contents, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

    if image is None:
        return {"success": False, "msg": "å›¾åƒè§£ç å¤±è´¥"}

    h_img, w_img, _ = image.shape

    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # --- æ­¥éª¤ 1: è¿è¡Œ AI æ¨¡å‹ ---
    # MediaPipe å§¿æ€æ£€æµ‹
    mp_results = pose.process(image_rgb)
    # YOLO æ£€æµ‹ (ç”¨äºå¤šäººåŠæ¨ªç«–å±åˆ¤å®š)
    # yolo_results = yolo_model(image_rgb)
    # å¼ºåˆ¶æŒ‡å®šè®¾å¤‡å¹¶æ˜¾å¼è°ƒç”¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    yolo_model.to(device)
    yolo_results = yolo_model(image_rgb)
    result = yolo_results[0]

    yolo_box = None
    if len(result.boxes) > 0:
        # 1. æå–ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„ç‰©ä½“åæ ‡ [x1, y1, x2, y2]
        # .cpu().numpy() å°†æ•°æ®ä» GPU è½¬åˆ° CPU æ•°ç»„
        box_data = result.boxes.xyxy[0].cpu().numpy()

        # 2. èµ‹å€¼
        yolo_x1, yolo_y1, yolo_x2, yolo_y2 = box_data
        yolo_box = (yolo_x1/w_img, yolo_y1/h_img, yolo_x2/w_img, yolo_y2/h_img)

        print(f"æ£€æµ‹åˆ°äºº: {yolo_box}")
    else:
        timestamp = int(time.time())
        filename = f"debug_frames/fail_{timestamp}.jpg"
        cv2.imwrite(filename, image)
        print(f"âš ï¸ æ£€æµ‹å¤±è´¥ï¼Œå›¾ç‰‡å·²ä¿å­˜è‡³: {filename}")
        # print("æœªæ£€æµ‹åˆ°äºº")

    # --- æ­¥éª¤ 2: åˆå§‹åŒ–è¿”å›ç»“æ„ ---
    response_data = {
        "success": True,
        "scale": 1.0,
        "bbox": [0.0, 0.0, 0.0, 0.0],
        "center": [0.5, 0.5],
        "suggestions": [],
        "user_points": [],
        "suggested_orientation": "portrait"  # é»˜è®¤ä¸ºç«–å±ï¼Œä¸è§¦å‘åŠ¨ç”»
    }

    # --- æ­¥éª¤ 3: æ ¸å¿ƒé€»è¾‘è®¡ç®— ---

    # A. å§¿æ€ç‚¹ä¸æ„å›¾è®¡ç®— (å¦‚æœæœ‰ MediaPipe ç»“æœ)
    if mp_results.pose_landmarks:
        landmarks = mp_results.pose_landmarks.landmark
        response_data["user_points"] = [[lm.x, lm.y] for lm in landmarks]

        # è°ƒç”¨ä½ ç°æœ‰çš„å·¥å…·å‡½æ•°è®¡ç®— BBox å’Œ ç¼©æ”¾å»ºè®®
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ PoseExtractor().extract_from_frame è¿”å›ç¬¦åˆè¦æ±‚çš„æ ¼å¼
        pose_data = PoseExtractor().extract_from_frame(image)
        # bbox_result = compute_bbox_standard(image, pose_data, yolo_model)
        bbox_result = compute_bbox_by_mode(image, pose_data, yolo_box, mode=view_mode)
        camera_advice = analyze_crop_and_zoom(image, pose_data, yolo_box)

        response_data["bbox"] = bbox_result["bbox"]  # [xmin, ymin, xmax, ymax] å½’ä¸€åŒ–
        response_data["center"] = bbox_result["target_center"]
        response_data["scale"] = bbox_result["scale"]

        # å°†åŸæœ‰å»ºè®®è½¬åŒ–ä¸º SuggestionDetail å¯¹è±¡åˆ—è¡¨æ ¼å¼ï¼ˆåŒ¹é… Compose ç«¯ï¼‰
        response_data["suggestions"] = camera_advice

    else:
        response_data["suggestions"] = [{"id": "0", "text": "æœªæ£€æµ‹åˆ°äººç‰©", "needModify": True}]

    # B. æ¨ªç«–å±åˆ¤å®šé€»è¾‘ (åŸºäº YOLO å¤šäººç»“æœ)
    # è°ƒç”¨ä½ å®šä¹‰çš„å»ºè®®å‡½æ•°
    # orientation, o_reason = suggest_orientation_multi(yolo_results)

    # å¦‚æœå»ºè®®æ˜¯æ¨ªå±ï¼Œåˆ™æ›´æ–°å­—æ®µï¼Œè§¦å‘å‰ç«¯â€œéœ‡åŠ¨â€å’Œâ€œç®­å¤´â€
    # if orientation == "æ¨ªå±":
    #     response_data["suggested_orientation"] = "landscape"
    #     response_data["suggestions"].append(
    #         {
    #         "id": "orient_01",
    #         "text": f"å»ºè®®åˆ‡æ¢æ¨ªå±: {o_reason}",
    #         "needModify": True
    #         }
    #     )
    # else:
    #     response_data["suggested_orientation"] = "portrait"

    response_data["suggested_orientation"] = "landscape"
    response_data["suggestions"].append(
        {
            "id": "orient_01",
            "text": f"å»ºè®®åˆ‡æ¢æ¨ªå±ã€‚",
            "needModify": True
        }
    )

    # --- æ­¥éª¤ 4: æ€§èƒ½ç»Ÿè®¡ä¸è¿”å› ---
    process_time = (time.time() - start_time) * 1000
    # print(f"â±ï¸ å§¿æ€åˆ†æè€—æ—¶: {process_time:.2f} ms")

    print(response_data)

    return response_data


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)